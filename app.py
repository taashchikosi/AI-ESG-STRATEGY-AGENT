# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GnJbcrfCz5Z_wfG3A5ogioXntL693XfW
"""

#!pip install -U langchain langchain-core langchain-community langchain-openai langchain-text-splitters faiss-cpu pypdf sentence-transformers tiktoken

import os, glob

# Loaders & Vector store
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import FAISS

# Offline embeddings (no API limits for building index)
from langchain_community.embeddings import HuggingFaceEmbeddings

# Chunking
from langchain_text_splitters import RecursiveCharacterTextSplitter

# LCEL (modern RAG pipeline)
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableParallel, RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# Answering model (OpenAI for final answers)
from langchain_openai import ChatOpenAI

# ==== üîê Secure OpenAI API key handling (for Streamlit Cloud) ====
import streamlit as st

# Retrieve the API key from Streamlit Cloud Secrets
api_key = st.secrets.get("OPENAI_API_KEY")

# Stop execution if key is missing
if not api_key:
    st.error("üö® Missing OpenAI API key. Please add it in Streamlit Cloud ‚Üí Settings ‚Üí Secrets.")
    st.stop()

# Set the key as an environment variable so LangChain / OpenAI can use it
os.environ["OPENAI_API_KEY"] = api_key
# ==== End secure block ====

#import os
#from getpass import getpass

#os.environ["OPENAI_API_KEY"] = getpass("Enter your OpenAI API key: ")

#from google.colab import drive
#drive.mount('/content/drive')

#BASE_DIR = "/content/drive/MyDrive/AI ESG Strategy Portfolio Project"  # <-- change if needed

#!ls "/content/drive/MyDrive/AI ESG Strategy Portfolio Project"

import glob

# -----------------------------------------------
# ‚úÖ LOAD EXISTING FAISS INDEX (Instead of creating a new one)
# -----------------------------------------------
import streamlit as st
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

# Define path to the FAISS index folder in your repo
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
FAISS_PATH = os.path.join(BASE_DIR, "esg_faiss_index")

# Initialize embeddings with your OpenAI key
embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

# Load your prebuilt FAISS index
db = FAISS.load_local(FAISS_PATH, embeddings, allow_dangerous_deserialization=True)
retriever = db.as_retriever(search_kwargs={"k": 3})

st.success("‚úÖ FAISS index loaded successfully from repo!")
# -----------------------------------------------


# Define the base directory (the current folder where your Streamlit app runs)
#BASE_DIR = os.path.dirname(os.path.abspath(__file__))

#files = glob.glob(f"{BASE_DIR}/**/*.pdf", recursive=True)
#print(f"Found {len(files)} PDF files:")
#for f in files[:10]:
    #print("-", f)

# ---- Load PDFs (recursive) ----
#docs = []
#pdf_paths = glob.glob(f"{BASE_DIR}/**/*.pdf", recursive=True)
#print(f"Found {len(pdf_paths)} PDFs")
#for p in pdf_paths:
    #try:
        #docs.extend(PyPDFLoader(p).load())
        #print("Loaded:", p)
    #except Exception as e:
        #print("Skipped:", p, "->", e)

#if not docs:
    #raise RuntimeError("No documents loaded. Check BASE_DIR path / file types.")

# ---- Split into chunks ----
#splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)
#chunks = splitter.split_documents(docs)
#print("Total chunks:", len(chunks))

# ---- Offline embeddings (fast, no rate limits) ----
#embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

# ---- Build FAISS & save ----
#vectorstore = FAISS.from_documents(chunks, embeddings)
#INDEX_DIR = "/content/esg_faiss_index"   # change if you prefer another path
#vectorstore.save_local(INDEX_DIR)
#print("‚úÖ Saved FAISS index to:", INDEX_DIR)

# ---- Load FAISS index & set up retriever ----

from langchain_community.vectorstores import FAISS

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
INDEX_DIR = os.path.join(BASE_DIR, "esg_faiss_index")

retriever = FAISS.load_local(
    INDEX_DIR,
    embeddings,
    allow_dangerous_deserialization=True
).as_retriever(search_kwargs={"k": 3})

# ---- Initialize the OpenAI model ----
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

# ---- Build LCEL RAG chain (no langchain.chains import needed) ----
prompt = ChatPromptTemplate.from_template("""
You are a helpful AI assistant analyzing ESG and Strategy portfolio reports.
Use the provided context to answer accurately, clearly, and concisely.

Context:
{context}

Question:
{question}
""")

from langchain_core.runnables import RunnableParallel, RunnablePassthrough

rag_chain = (
    RunnableParallel({"context": retriever, "question": RunnablePassthrough()})
    | prompt
    | llm
    | StrOutputParser()
)

# ---- Interactive question loop ----
print("\nüí¨ AI ESG & Strategy Agent Ready!")
#while True:
    #query = input("\nAsk a question about your documents (or type 'exit' to quit): ")
    #if query.lower() in ["exit", "quit"]:
        #print("üëã Exiting agent.")
        #break
    #print("\nüîç Searching the ESG & case study documents...\n")
    #answer = rag_chain.invoke(query)
    #print("=== üß† AI Response ===\n")
    #print(answer)

# -----------------------------------------------
# üí¨ Streamlit Chat Interface
# -----------------------------------------------
st.title("ü§ñ AI ESG & Strategy Agent")

# Input box for recruiter questions
user_query = st.text_input("Ask a question about ESG frameworks and/or strategy for implementation:")

if user_query:
    with st.spinner("Thinking..."):
        answer = rag_chain.invoke(user_query)
    st.markdown("### üß† AI Response")
    st.write(answer)

# Commented out IPython magic to ensure Python compatibility.
# %%writefile agent.py
# from langchain.vectorstores import FAISS
# from langchain.embeddings import OpenAIEmbeddings
# from langchain.chains import RetrievalQA
# from langchain.chat_models import ChatOpenAI
# import os
# import streamlit as st
# 
# @st.cache_resource(show_spinner=False)
# def init_pipeline():
#     """Initializes the FAISS retriever and LLM pipeline."""
#     embeddings = OpenAIEmbeddings(openai_api_key=os.getenv("OPENAI_API_KEY"))
#     db = FAISS.load_local("esg_faiss_index", embeddings, allow_dangerous_deserialization=True)
#     retriever = db.as_retriever(search_kwargs={"k": 3})
# 
#     llm = ChatOpenAI(temperature=0.2, openai_api_key=os.getenv("OPENAI_API_KEY"))
#     qa_chain = RetrievalQA.from_chain_type(
#         llm=llm,
#         retriever=retriever,
#         chain_type="stuff"
#     )
#     return qa_chain
# 
# def answer_query(question: str) -> str:
#     """Runs the query through the QA pipeline."""
#     qa = init_pipeline()
#     response = qa.run(question)
#     return response